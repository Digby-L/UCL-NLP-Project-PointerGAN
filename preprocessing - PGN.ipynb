{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "# nltk.download()\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Pytorch library for training\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "# from torchtext.data import Field, BucketIterator, Example\n",
    "\n",
    "#from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-trained word embedding by GolVe\n",
    "https://github.com/stanfordnlp/GloVe\n",
    "...Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download): glove.6B.zip\n",
    "Due to the nature of WikiHow dataset, we choose word embedding result file 'glove.6B' which is pre-trained on Wikipedia and Gigaword dataset. Besides, it contains four .text for different embdedding vector length: 50, 100, 200, 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-05-30 13:15:11--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-05-30 13:15:12--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "Unable to establish SSL connection.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# # Choose embedding dimention\n",
    "# embed_dim = 200\n",
    "\n",
    "# # Download and unzip GloVe embedding\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input your pre-train txt path and parse the data\n",
    "# path = './glove.6B/glove.6B.200d.txt'\n",
    "\n",
    "# embed_dict = {}\n",
    "# with open(path,'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for l in lines:\n",
    "#         w = l.split()[0]\n",
    "#         v = np.array(l.split()[1:]).astype('float')\n",
    "#         embed_dict[w] = v\n",
    "\n",
    "# embed_dict['@@_unknown_@@'] = np.random.random(embed_dim)\n",
    "\n",
    "# # remove all the unnecesary files\n",
    "# !rm -rf glove.6B.zip\n",
    "# !rm -rf glove.6B.50d.txt\n",
    "# !rm -rf glove.6B.100d.txt\n",
    "# !rm -rf glove.6B.200d.txt\n",
    "# !rm -rf glove.6B.300d.txt\n",
    "\n",
    "# # check the length of the dictionary\n",
    "# len(embed_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WikiHow Dataset\n",
    "Download dataset from https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_csv('.\\Dataset\\wikihowSep.csv')\n",
    "data = data[:1000]\n",
    "data = data.astype(str)\n",
    "rows, columns = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expand contraction by https://github.com/khurram6968/NLP-Expand-Contraction-Python/blob/master/NLP.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_map={\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd've\": \"how did have\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"might have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"shall'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"will't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"would't\": \"would not\",\n",
    "    \"would't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you have all\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "}\n",
    "\n",
    "def expand_contractions(sent, mapping):\n",
    "    #pattern for matching contraction with their expansions\n",
    "    pattern = re.compile('({})'.format('|'.join(mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_map(contraction):\n",
    "        #using group method to access subgroups of the match\n",
    "        match = contraction.group(0)\n",
    "        #to retain correct case of the word\n",
    "        first_char = match[0]\n",
    "        #find out the expansion\n",
    "        expansion = mapping.get(match) if mapping.get(match) else mapping.get(match.lower())\n",
    "        expansion = first_char + expansion[1:]\n",
    "        return expansion\n",
    "    #using sub method to replace all contractions with their expansions for a sentence\n",
    "    #function expand_map will be called for every non overlapping occurence of the pattern\n",
    "    expand_sent = pattern.sub(expand_map, sent)\n",
    "    return expand_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataframe, target_col): \n",
    "    # Extraction from dataframe into a list\n",
    "    text = [article for article in getattr(dataframe, target_col)]\n",
    "    \n",
    "    # Removing accented characters\n",
    "    text = [unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf-8', 'ignore') for sentence in text]\n",
    "    \n",
    "    # Expanding contractions\n",
    "    text = [expand_contractions(sentence, contraction_map) for sentence in text]\n",
    "\n",
    "    # Removing special characters\n",
    "    pat1 = r'[^a-zA-z0-9.,!?\\s]' \n",
    "    # pat1 = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    text = [re.sub(pat1, '', sentence) for sentence in text]\n",
    "    \n",
    "    # Removing extra commas\n",
    "    pat2 = r'[.]+[\\n]+[,]'\n",
    "    text = [re.sub(pat2,\".\\n\", sentence) for sentence in text]\n",
    "    \n",
    "    # Removing extra whitespaces and tabs\n",
    "    # pat3 = r'^\\s*|\\s\\s*'\n",
    "    pat3 = r'^\\s+$|\\s+$'\n",
    "    text = [re.sub(pat3, '', sentence).strip() for sentence in text]\n",
    "    \n",
    "    # Add space before '.'\n",
    "    pat4 = r'\\.|\\?|\\ÔºÅ|\\,'\n",
    "    text = [re.sub(pat4, ' ', sentence) for sentence in text]\n",
    "    \n",
    "    # Lowercase\n",
    "    text = [sentence.lower() for sentence in text]\n",
    "    \n",
    "    # Tokenize\n",
    "    text = [('sos ' + sentence + ' eos').split() for sentence in text]\n",
    "    \n",
    "    return np.array(text, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data = data_loader(data, 'text')\n",
    "headline_data = data_loader(data, 'headline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove text and headline that are too long or too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_word'] = [len(text_data[i]) for i in range(text_data.shape[0])]\n",
    "num_word = np.sort(data['num_word'])\n",
    "\n",
    "data['num_word_hl'] = [len(headline_data[i]) for i in range(headline_data.shape[0])]\n",
    "num_word_hl = np.sort(data['num_word_hl'])\n",
    "\n",
    "min_text_len = num_word[int(len(num_word)*0.1)]\n",
    "max_text_len = num_word[int(len(num_word)*0.95)]\n",
    "\n",
    "min_hl_len = num_word_hl[int(len(num_word_hl)*0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    }
   ],
   "source": [
    "headline_ratio_threshold = 0.75\n",
    "\n",
    "del_idx = []\n",
    "for i in range(rows):\n",
    "    # Remove too short text\n",
    "    if data['num_word'][i] < min_text_len:\n",
    "        del_idx.append(i)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Remove too long text\n",
    "    if max_text_len < data['num_word'][i]:\n",
    "        del_idx.append(i)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Remove headline longer than 0.75*len(text)\n",
    "    if data['num_word_hl'][i] > headline_ratio_threshold*data['num_word'][i]:\n",
    "        del_idx.append(i)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Remove too short headline\n",
    "    if data['num_word_hl'][i] < min_hl_len:\n",
    "        del_idx.append(i)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# Number of removed index\n",
    "print(len(np.unique(del_idx)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.delete(text_data, del_idx, axis=0)\n",
    "headline_data = np.delete(headline_data, del_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(754, 754)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.shape[0], headline_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train, test, validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, headline_train, headline_test = train_test_split(text_data, headline_data, test_size=0.1, random_state=1)\n",
    "\n",
    "text_train, text_dev, headline_train, headline_dev = train_test_split(text_train, headline_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort sentence from longer to shorter length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sorter(text, headline): \n",
    "    headline = [y for x,y in sorted(zip(text, headline), key = lambda pair: len(pair[0]), reverse = True)]\n",
    "    text = list(text)\n",
    "    text.sort(key = lambda x: len(x), reverse = True)\n",
    "\n",
    "    return np.array(text), np.array(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-515ae7e24249>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(text), np.array(headline)\n"
     ]
    }
   ],
   "source": [
    "text_train, headline_train = data_sorter(text_train, headline_train)\n",
    "text_test,  headline_test  = data_sorter(text_test, headline_test)\n",
    "text_dev,   headline_dev   = data_sorter(text_dev, headline_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "np.save('./Dataset3/text_train.npy', text_train)\n",
    "np.save('./Dataset3/headline_train.npy', headline_train)\n",
    "\n",
    "# dev\n",
    "np.save('./Dataset3/text_dev.npy', text_dev)\n",
    "np.save('./Dataset3/headline_dev.npy', headline_dev)\n",
    "\n",
    "# test\n",
    "np.save('./Dataset3/text_test.npy', text_test)\n",
    "np.save('./Dataset3/headline_test.npy', headline_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary\n",
    "https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    PAD_token = 0   # Used for padding short sentences\n",
    "    SOS_token = 1   # Start-of-sentence token\n",
    "    EOS_token = 2   # End-of-sentence token\n",
    "    UNK_token = 3   # Out-of-vocabulary token\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"pad\":0, \"sos\":1, \"eos\":2, \"unk\":3}\n",
    "        self.word2count = {\"pad\":0, \"sos\":0, \"eos\":0, \"unk\":0}              \n",
    "        self.index2word = {0: \"pad\", 1: \"sos\", 2: \"eos\", 3: \"unk\"}\n",
    "        self.num_words = 4\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        for word in sentence:           \n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            self.longest_sentence = sentence_len\n",
    "        self.num_sentences += 1\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocabulary = Vocabulary('text')\n",
    "headline_vocabulary = Vocabulary('headline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text and headline vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text vocabulary\n",
    "for i in range(len(text_train)):\n",
    "    text_vocabulary.add_sentence(text_train[i])\n",
    "    for word in text_train[i]:\n",
    "        text_vocabulary.add_word(word)\n",
    "text_train_vocab_len = len(text_vocabulary.word2index.keys())\n",
    "    \n",
    "# Index test and dev dataset using text_vocabulary\n",
    "for i in range(len(text_dev)):\n",
    "    for word in text_dev[i]:\n",
    "        text_vocabulary.add_word(word)\n",
    "for i in range(len(text_test)):\n",
    "    for word in text_test[i]:\n",
    "        text_vocabulary.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate headline vocabulary\n",
    "for i in range(len(headline_train)):\n",
    "    headline_vocabulary.add_sentence(headline_train[i])\n",
    "    for word in headline_train[i]:\n",
    "        headline_vocabulary.add_word(word)\n",
    "hl_train_vocab_len = len(headline_vocabulary.word2index.keys())\n",
    "    \n",
    "    \n",
    "# Index test and dev dataset using headline_vocabulary\n",
    "for i in range(len(headline_dev)):\n",
    "    for word in headline_dev[i]:\n",
    "        headline_vocabulary.add_word(word)\n",
    "for i in range(len(headline_test)):\n",
    "    for word in headline_test[i]:\n",
    "        headline_vocabulary.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6492\n",
      "1396\n",
      "7239\n",
      "1593\n"
     ]
    }
   ],
   "source": [
    "print(text_train_vocab_len)\n",
    "print(hl_train_vocab_len)\n",
    "\n",
    "print(len(text_vocabulary.word2index.keys()))\n",
    "print(len(headline_vocabulary.word2index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word to index and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2in(data,vocabulary):\n",
    "    data_idx = []\n",
    "    lengths = []\n",
    "    for i in range(len(data)):\n",
    "        idx = []\n",
    "        lengths.append(len(data[i]))\n",
    "        for word in data[i]:\n",
    "            idx.append(vocabulary.to_index(word))\n",
    "        data_idx.append(torch.tensor(idx))\n",
    "    lengths = torch.tensor(lengths)\n",
    "    data_pad = torch.nn.utils.rnn.pad_sequence(data_idx, batch_first=True, padding_value=0.0)\n",
    "    return data_idx, data_pad, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchpad_data(data):\n",
    "    voc = Vocabulary('text')\n",
    "    data_idx = []\n",
    "    for i in range(len(data)):\n",
    "        idx = []\n",
    "        for word in data[i]:\n",
    "            voc.add_word(word)\n",
    "        for word in data[i]:\n",
    "            idx.append(voc.to_index(word))\n",
    "        data_idx.append(torch.tensor(idx))\n",
    "    data_pad = torch.nn.utils.rnn.pad_sequence(data_idx, batch_first=True, padding_value=0.0)\n",
    "    return data_idx, data_pad, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data_idx, data_pad and data_lengths\n",
    "text_train_idx, text_train_pad, text_train_lengths = w2in(text_train,text_vocabulary)\n",
    "headline_train_idx, headline_train_pad, headline_train_lengths = w2in(headline_train,headline_vocabulary)\n",
    "\n",
    "text_dev_idx, text_dev_pad, text_dev_lengths = w2in(text_dev,text_vocabulary)\n",
    "headline_dev_idx, headline_dev_pad, headline_dev_lengths = w2in(headline_dev, headline_vocabulary)\n",
    "\n",
    "text_test_idx, text_test_pad, text_test_lengths = w2in(text_test,text_vocabulary)\n",
    "headline_test_idx, headline_test_pad, headline_test_lengths = w2in(headline_test, headline_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pad_mask of each dataset (position with padding = 1, no padding = 0)\n",
    "\n",
    "# Training data pad_mask\n",
    "text_train_padmask = torch.where(text_train_pad == 0, 1, 0)\n",
    "headline_train_padmask = torch.where(headline_train_pad == 0, 1, 0)\n",
    "\n",
    "# Dev data pad_mask\n",
    "text_dev_padmask = torch.where(text_dev_pad == 0, 1, 0)\n",
    "headline_dev_padmask = torch.where(headline_dev_pad == 0, 1, 0)\n",
    "\n",
    "# Testing data pad_mask\n",
    "text_test_padmask = torch.where(text_test_pad == 0, 1, 0)\n",
    "headline_test_padmask = torch.where(headline_test_pad == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text_train[-1])\n",
    "# print(text_test_pad[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of vocabulary lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of OOV in each sentence in the train set: suppose to be 0 for every sentence\n",
    "text_train_oov = torch.sum(torch.where(text_train_pad > text_train_vocab_len, 1, 0), axis = 1)\n",
    "headline_train_oov = torch.sum(torch.where(headline_train_pad > hl_train_vocab_len, 1, 0), axis = 1)\n",
    "\n",
    "# print(text_train_oov, headline_train_oov)\n",
    "\n",
    "# number of OOV in each sentence in the dev set\n",
    "text_dev_oov = torch.sum(torch.where(text_dev_pad > text_train_vocab_len, 1, 0), axis = 1)\n",
    "headline_dev_oov = torch.sum(torch.where(headline_dev_pad > hl_train_vocab_len, 1, 0), axis = 1)\n",
    "\n",
    "# number of OOV in each sentence in the test set\n",
    "text_test_oov = torch.sum(torch.where(text_test_pad > text_train_vocab_len, 1, 0), axis = 1)\n",
    "headline_test_oov = torch.sum(torch.where(headline_test_pad > hl_train_vocab_len, 1, 0), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset without OOV - set UNK index to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data: no 3\n",
    "text_train_no = torch.where(text_train_pad > text_train_vocab_len, 3, text_train_pad)\n",
    "headline_train_no = torch.where(headline_train_pad > hl_train_vocab_len, 3, headline_train_pad)\n",
    "\n",
    "# dev data\n",
    "text_dev_no = torch.where(text_dev_pad > text_train_vocab_len, 3, text_dev_pad)\n",
    "headline_dev_no = torch.where(headline_dev_pad > hl_train_vocab_len, 3, headline_dev_pad)\n",
    "\n",
    "# test data\n",
    "text_test_no = torch.where(text_test_pad > text_train_vocab_len, 3, text_test_pad)\n",
    "headline_test_no = torch.where(headline_test_pad > hl_train_vocab_len, 3, headline_test_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form datasets containing data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zip text and headline together for dataloader\n",
    "traindata_zip = torch.utils.data.TensorDataset(text_train_pad, headline_train_pad, \n",
    "                                               text_train_padmask, headline_train_padmask, \n",
    "                                               text_train_lengths, headline_train_lengths,\n",
    "                                               text_train_oov, headline_train_oov,\n",
    "                                               text_train_no, headline_train_no)\n",
    "\n",
    "devdata_zip = torch.utils.data.TensorDataset(text_dev_pad, headline_dev_pad,\n",
    "                                             text_dev_padmask, headline_dev_padmask, \n",
    "                                             text_dev_lengths, headline_dev_lengths,\n",
    "                                             text_dev_oov, headline_dev_oov,\n",
    "                                             text_dev_no, headline_dev_no)\n",
    "\n",
    "testdata_zip = torch.utils.data.TensorDataset(text_test_pad, headline_test_pad,\n",
    "                                              text_test_padmask, headline_test_padmask, \n",
    "                                              text_test_lengths, headline_test_lengths,\n",
    "                                              text_test_oov, headline_test_oov,\n",
    "                                              text_test_no, headline_test_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save datasets and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save datasets\n",
    "# train data\n",
    "torch.save(traindata_zip, './Dataset3/traindata_zip.pt')\n",
    "\n",
    "# dev data\n",
    "torch.save(devdata_zip, './Dataset3/devdata_zip.pt')\n",
    "\n",
    "# test data\n",
    "torch.save(testdata_zip, './Dataset3/testdata_zip.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data by\n",
    "# temp = torch.load('./Dataset3/traindata_zip.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save vocabulary\n",
    "# train vocab\n",
    "torch.save(text_vocabulary, './Dataset3/text_vocabulary.pt')\n",
    "torch.save(headline_vocabulary, './Dataset3/headline_vocabulary.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set batch size and split data after padding to batches\n",
    "def batch_dataloader(data, Batch_size):\n",
    "    data_dataloader = torch.utils.data.DataLoader(data, batch_size=Batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return data_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data batching\n",
    "trainloader = batch_dataloader(traindata_zip, 100)\n",
    "\n",
    "devloader = batch_dataloader(devdata_zip, 20)\n",
    "\n",
    "testloader = batch_dataloader(testdata_zip, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7239"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_vocabulary.index2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
