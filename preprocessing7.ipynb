{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Pytorch library for training\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, Example\n",
    "\n",
    "#from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-trained word embedding by GolVe\n",
    "https://github.com/stanfordnlp/GloVe\n",
    "...Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download): glove.6B.zip\n",
    "Due to the nature of WikiHow dataset, we choose word embedding result file 'glove.6B' which is pre-trained on Wikipedia and Gigaword dataset. Besides, it contains four .text for different embdedding vector length: 50, 100, 200, 300. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load WikiHow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_csv('.\\Dataset\\wikihowSep.csv')\n",
    "data = data[:1000]\n",
    "data = data.astype(str)\n",
    "rows, columns = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expand contraction by https://github.com/khurram6968/NLP-Expand-Contraction-Python/blob/master/NLP.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_map={\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd've\": \"how did have\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"might have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"shall'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"will't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"would't\": \"would not\",\n",
    "    \"would't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you have all\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "}\n",
    "\n",
    "def expand_contractions(sent, mapping):\n",
    "    #pattern for matching contraction with their expansions\n",
    "    pattern = re.compile('({})'.format('|'.join(mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_map(contraction):\n",
    "        #using group method to access subgroups of the match\n",
    "        match = contraction.group(0)\n",
    "        #to retain correct case of the word\n",
    "        first_char = match[0]\n",
    "        #find out the expansion\n",
    "        expansion = mapping.get(match) if mapping.get(match) else mapping.get(match.lower())\n",
    "        expansion = first_char + expansion[1:]\n",
    "        return expansion\n",
    "    #using sub method to replace all contractions with their expansions for a sentence\n",
    "    #function expand_map will be called for every non overlapping occurence of the pattern\n",
    "    expand_sent = pattern.sub(expand_map, sent)\n",
    "    return expand_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple pre-processing by https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discard unnecessary data, due to computational resource limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_word'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "num_word = np.sort(data['num_word'].values)\n",
    "\n",
    "data['num_word_hl'] = data['headline'].apply(lambda x: len(str(x).split()))\n",
    "num_word_hl = np.sort(data['num_word_hl'].values)\n",
    "\n",
    "min_text_len = num_word[int(len(num_word)*0.1)]\n",
    "max_text_len = num_word[int(len(num_word)*0.95)]\n",
    "\n",
    "min_hl_len = num_word_hl[int(len(num_word_hl)*0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_ratio_threshold = 0.75\n",
    "\n",
    "del_idx = []\n",
    "for i in range(rows):\n",
    "#     if data['num_word'][i] < max_text_len and data['num_word'][i] > min_text_len:\n",
    "    if data['num_word'][i] < min_text_len:\n",
    "        del_idx.append(i)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if max_text_len < data['num_word'][i]:\n",
    "        del_idx.append(i)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if data['num_word_hl'][i] > headline_ratio_threshold*data['num_word'][i]:\n",
    "        del_idx.append(i)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if data['num_word_hl'][i] < min_hl_len:\n",
    "        del_idx.append(i)\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(del_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.drop(del_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(755, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataframe, target_col): \n",
    "    # extraction from dataframe in to list\n",
    "    text = [article for article in getattr(dataframe, target_col)]\n",
    "    \n",
    "    # Removing Accented Characters\n",
    "    text = [unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf-8', 'ignore') for sentence in text]\n",
    "    \n",
    "    # Expanding Contractions\n",
    "    text = [expand_contractions(sentence, contraction_map) for sentence in text]\n",
    "\n",
    "    # Removing Special Characters\n",
    "    pat1 = r'[^a-zA-z0-9.,!?\\s]' \n",
    "    # pat1 = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    text = [re.sub(pat1, '', sentence) for sentence in text]\n",
    "    \n",
    "    # Removing Extra Commas\n",
    "    pat2 = r'[.]+[\\n]+[,]'\n",
    "    text = [re.sub(pat2,\".\\n\", sentence) for sentence in text]\n",
    "    \n",
    "    # Removing extra whitespaces and tabs\n",
    "    # pat3 = r'^\\s*|\\s\\s*'\n",
    "    pat3 = r'^\\s+$|\\s+$'\n",
    "    text = [re.sub(pat3, '', sentence).strip() for sentence in text]\n",
    "    \n",
    "    # Add space before .\n",
    "    pat4 = r'\\.|\\?|\\！'\n",
    "    text = [re.sub(pat4, ' . ', sentence) for sentence in text]\n",
    "    \n",
    "    # Lowercase\n",
    "    text = [sentence.lower() for sentence in text]\n",
    "    \n",
    "    # tokenize\n",
    "    text = [('sos ' + sentence + ' eos').split() for sentence in text]\n",
    "    \n",
    "    return np.array(text, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 523 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_data = data_loader(data_new, 'text')\n",
    "headline_data = data_loader(data_new, 'headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, test, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, headline_train, headline_test = train_test_split(text_data, headline_data, test_size=0.1, random_state=1)\n",
    "\n",
    "text_train, text_dev, headline_train, headline_dev = train_test_split(text_train, headline_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort sentence from longer to shorter length, for more efficient processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sorter(text, headline): \n",
    "    headline = [y for x,y in sorted(zip(text, headline), key = lambda pair: len(pair[0]), reverse = True)]\n",
    "    text = list(text)\n",
    "    text.sort(key = lambda x: len(x), reverse = True)\n",
    "\n",
    "    return np.array(text), np.array(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-515ae7e24249>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(text), np.array(headline)\n"
     ]
    }
   ],
   "source": [
    "text_train, headline_train = data_sorter(text_train, headline_train)\n",
    "text_test,  headline_test  = data_sorter(text_test, headline_test)\n",
    "text_dev,   headline_dev   = data_sorter(text_dev, headline_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "np.save('./Dataset3/text_train.npy', text_train)\n",
    "np.save('./Dataset3/headline_train.npy', headline_train)\n",
    "\n",
    "# dev\n",
    "np.save('./Dataset3/text_dev.npy', text_dev)\n",
    "np.save('./Dataset3/headline_dev.npy', headline_dev)\n",
    "\n",
    "# test\n",
    "np.save('./Dataset3/text_test.npy', text_test)\n",
    "np.save('./Dataset3/headline_test.npy', headline_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary\n",
    "https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAD_token = 0   # Used for padding short sentences\n",
    "# SOS_token = 1   # Start-of-sentence token\n",
    "# EOS_token = 2   # End-of-sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    PAD_token = 0   # Used for padding short sentences\n",
    "    SOS_token = 1   # Start-of-sentence token\n",
    "    EOS_token = 2   # End-of-sentence token\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "#         self.index2word = {PAD_token: 'pad', SOS_token: 'sos', EOS_token: 'eos'}\n",
    "        self.index2word = {0: \"pad\", 1: \"sos\", 2: \"eos\"}\n",
    "        self.num_words = 3\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split(' '):\n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            self.longest_sentence = sentence_len\n",
    "        self.num_sentences += 1\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchpad_data(data):\n",
    "    voc = Vocabulary('text')\n",
    "    data_idx = []\n",
    "    for i in range(len(data)):\n",
    "        idx = []\n",
    "        for word in data[i]:\n",
    "            voc.add_word(word)\n",
    "    \n",
    "        for word in data[i]:\n",
    "            idx.append(voc.to_index(word))\n",
    "\n",
    "        data_idx.append(torch.tensor(idx))\n",
    "        \n",
    "    data_pad = torch.nn.utils.rnn.pad_sequence(data_idx, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    return data_idx, data_pad, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training padding\n",
    "text_train_idx, text_train_pad, text_train_voc = batchpad_data(text_train)\n",
    "headline_train_idx, headline_train_pad, headline_train_voc= batchpad_data(headline_train)\n",
    "\n",
    "## Validation padding\n",
    "text_dev_idx, text_dev_pad, text_dev_voc = batchpad_data(text_dev)\n",
    "headline_dev_idx, headline_dev_pad, headline_dev_voc = batchpad_data(headline_dev)\n",
    "\n",
    "## Testing padding\n",
    "text_test_idx, text_test_pad, text_test_voc = batchpad_data(text_test)\n",
    "headline_test_idx, headline_test_pad, headline_test_voc = batchpad_data(headline_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([611, 207])\n",
      "torch.Size([611, 52])\n",
      "7559\n"
     ]
    }
   ],
   "source": [
    "print(text_train_pad.shape)\n",
    "print(headline_train_pad.shape)\n",
    "print(len(text_train_voc.word2index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sos',\n",
       " 'studies',\n",
       " 'have',\n",
       " 'shown',\n",
       " 'that',\n",
       " 'eating',\n",
       " 'two',\n",
       " 'servings',\n",
       " 'of',\n",
       " 'seafood',\n",
       " 'every',\n",
       " 'week',\n",
       " 'can',\n",
       " 'help',\n",
       " 'child',\n",
       " 'development',\n",
       " 'and',\n",
       " 'even',\n",
       " 'result',\n",
       " 'in',\n",
       " 'higher',\n",
       " 'iqs',\n",
       " '.',\n",
       " 'eos']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   3,  393, 1206, 1354,   12,   13,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_train_pad[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zip text and headline together for dataloader\n",
    "traindata = torch.utils.data.TensorDataset(text_train_pad, headline_train_pad)\n",
    "\n",
    "devdata = torch.utils.data.TensorDataset(text_dev_pad, headline_dev_pad)\n",
    "\n",
    "testdata = torch.utils.data.TensorDataset(text_test_pad, headline_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set batch size and split data after padding to batches\n",
    "def batch_dataloader(data, Batch_size):\n",
    "    data_dataloader = torch.utils.data.DataLoader(data, batch_size=Batch_size, shuffle=False, num_workers=0)\n",
    "#     for i in data_dataloader:\n",
    "#         i = torch.transpose(i, 0 ,1)\n",
    "#         print(data_dataloader)\n",
    "    \n",
    "    return data_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data batching\n",
    "trainloader = batch_dataloader(traindata, 15)\n",
    "\n",
    "devloader = batch_dataloader(devdata, 2)\n",
    "\n",
    "testloader = batch_dataloader(testdata, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transpose dataset for training\n",
    "text_testrun, hl_testrun = next(iter(trainloader))\n",
    "\n",
    "text_testrun = torch.transpose(text_testrun, 0 ,1)\n",
    "hl_testrun = torch.transpose(hl_testrun, 0 ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_testrun = text_testrun.to(device)\n",
    "hl_testrun = hl_testrun.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7562"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_train_voc.index2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([207, 15])\n",
      "torch.Size([52, 15])\n"
     ]
    }
   ],
   "source": [
    "print(text_testrun.shape)\n",
    "print(hl_testrun.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set parameters\n",
    "# input_size = int(torch.max(text_train_pad) + 1)\n",
    "# output_size = int(torch.max(headline_train_pad) + 1)\n",
    "\n",
    "input_size = int(len(text_train_voc.index2word.keys())+1)\n",
    "output_size = int(len(headline_train_voc.index2word.keys())+1)\n",
    "\n",
    "enc_emb_size = 256\n",
    "dec_emb_size = 256\n",
    "hid_size = 128\n",
    "\n",
    "n_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hid_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hid_size, n_layers, dropout=dropout)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(emb_size, hid_size, n_layers, dropout=dropout, bidirectional = True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 15, 128]), torch.Size([2, 15, 128]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(input_size, enc_emb_size, hid_size, n_layers, enc_dropout).to(device)\n",
    "\n",
    "hidden, cell = encoder(text_testrun)\n",
    "hidden.shape, cell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size, hid_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hid_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hid_size, output_size, bias = True)\n",
    "\n",
    "    def forward(self, output, hidden, cell):\n",
    "        embedded = self.embedding(output.unsqueeze(0))\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        \n",
    "        prediction = self.out(outputs.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 1414]), torch.Size([2, 15, 128]), torch.Size([2, 15, 128]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(output_size, dec_emb_size, hid_size, n_layers, dec_dropout).to(device)\n",
    "\n",
    "prediction, hidden, cell = decoder(hl_testrun[0], hidden, cell)\n",
    "prediction.shape, hidden.shape, cell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not yet debugged\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim, bias=False)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, s, enc_output):\n",
    "        \n",
    "        # s = [batch_size, dec_hid_dim]\n",
    "        # enc_output = [src_len, batch_size, enc_hid_dim * 2]\n",
    "        \n",
    "        batch_size = enc_output.shape[1]\n",
    "        src_len = enc_output.shape[0]\n",
    "        \n",
    "        # repeat decoder hidden state src_len times\n",
    "        # s = [batch_size, src_len, dec_hid_dim]\n",
    "        # enc_output = [batch_size, src_len, enc_hid_dim * 2]\n",
    "        s = s.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        enc_output = enc_output.transpose(0, 1)\n",
    "        \n",
    "        # energy = [batch_size, src_len, dec_hid_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((s, enc_output), dim = 2)))\n",
    "        \n",
    "        # attention = [batch_size, src_len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, text_batch, headline_batch, teacher_forcing_ratio: float=0.5):\n",
    "        \n",
    "#         text_batch = torch.transpose(text_batch, 0, 1)\n",
    "#         headline_batch = torch.transpose(headline_batch, 0, 1)\n",
    "\n",
    "        max_len, batch_size = headline_batch.shape\n",
    "        headline_vocab_size = self.decoder.output_size\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, headline_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "        hidden, cell = self.encoder(text_batch)\n",
    "\n",
    "        for i in range(1, max_len):\n",
    "            prediction, hidden, cell = self.decoder(headline_batch[0], hidden, cell)\n",
    "            outputs[i] = prediction\n",
    "\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                hl_batch_i = headline_batch[i]\n",
    "            else:\n",
    "                hl_batch_i = prediction.argmax(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7563, 256)\n",
       "    (lstm): LSTM(256, 128, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(1414, 256)\n",
       "    (lstm): LSTM(256, 128, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=128, out_features=1414, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that this implementation assumes that the size of the hidden layer,\n",
    "# and the number of layer are the same between the encoder and decoder\n",
    "encoder = Encoder(input_size, enc_emb_size, hid_size, n_layers, enc_dropout)\n",
    "decoder = Decoder(output_size, dec_emb_size, hid_size, n_layers, dec_dropout)\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
    "seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 15, 1414])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs = seq2seq(next(iter(text_trainloader)), next(iter(headline_trainloader)))\n",
    "outputs = seq2seq(text_testrun, hl_testrun)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq.parameters())\n",
    "\n",
    "# ignore the padding = 0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq, trainloader, optimizer, criterion):\n",
    "    seq2seq.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for text_batch, hl_batch in trainloader:\n",
    "        text_batch = torch.transpose(text_batch, 0, 1)\n",
    "        hl_batch = torch.transpose(hl_batch, 0, 1)\n",
    "        \n",
    "        ## send to cuda\n",
    "        text_batch = text_batch.to(device)\n",
    "        hl_batch = hl_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = seq2seq(text_batch, hl_batch)\n",
    "        outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        \n",
    "        hl_flatten = hl_batch[1:].reshape(-1)\n",
    "        loss = criterion(outputs_flatten, hl_flatten)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(seq2seq, trainloader, criterion):\n",
    "    seq2seq.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, hl_batch in trainloader:\n",
    "            text_batch = torch.transpose(text_batch, 0, 1)\n",
    "            hl_batch = torch.transpose(hl_batch, 0, 1)\n",
    "            \n",
    "            ## send to cuda\n",
    "            text_batch = text_batch.to(device)\n",
    "            hl_batch = hl_batch.to(device)\n",
    "            \n",
    "            # teacher forcing not used\n",
    "            outputs = seq2seq(text_batch, hl_batch, teacher_forcing_ratio=0) \n",
    "            outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            \n",
    "            hl_flatten = hl_batch[1:].reshape(-1)\n",
    "            loss = criterion(outputs_flatten, hl_flatten)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 7s\n",
      "\tTrain Loss: 6.048 | Train PPL: 423.063\n",
      "\t Val. Loss: 6.752 |  Val. PPL: 855.906\n",
      "Epoch: 02 | Time: 0m 8s\n",
      "\tTrain Loss: 5.315 | Train PPL: 203.432\n",
      "\t Val. Loss: 6.808 |  Val. PPL: 905.089\n",
      "Epoch: 03 | Time: 0m 8s\n",
      "\tTrain Loss: 5.200 | Train PPL: 181.329\n",
      "\t Val. Loss: 6.933 |  Val. PPL: 1025.794\n",
      "Epoch: 04 | Time: 0m 8s\n",
      "\tTrain Loss: 5.124 | Train PPL: 168.066\n",
      "\t Val. Loss: 7.010 |  Val. PPL: 1108.034\n",
      "Epoch: 05 | Time: 0m 8s\n",
      "\tTrain Loss: 5.063 | Train PPL: 158.121\n",
      "\t Val. Loss: 7.046 |  Val. PPL: 1148.292\n",
      "Epoch: 06 | Time: 0m 8s\n",
      "\tTrain Loss: 5.009 | Train PPL: 149.819\n",
      "\t Val. Loss: 7.109 |  Val. PPL: 1223.117\n",
      "Epoch: 07 | Time: 0m 8s\n",
      "\tTrain Loss: 4.969 | Train PPL: 143.844\n",
      "\t Val. Loss: 7.162 |  Val. PPL: 1289.159\n",
      "Epoch: 08 | Time: 0m 8s\n",
      "\tTrain Loss: 4.931 | Train PPL: 138.507\n",
      "\t Val. Loss: 7.201 |  Val. PPL: 1340.913\n",
      "Epoch: 09 | Time: 0m 8s\n",
      "\tTrain Loss: 4.900 | Train PPL: 134.333\n",
      "\t Val. Loss: 7.261 |  Val. PPL: 1423.183\n",
      "Epoch: 10 | Time: 0m 8s\n",
      "\tTrain Loss: 4.875 | Train PPL: 130.986\n",
      "\t Val. Loss: 7.293 |  Val. PPL: 1470.691\n",
      "Epoch: 11 | Time: 0m 9s\n",
      "\tTrain Loss: 4.852 | Train PPL: 127.988\n",
      "\t Val. Loss: 7.339 |  Val. PPL: 1539.145\n",
      "Epoch: 12 | Time: 0m 8s\n",
      "\tTrain Loss: 4.830 | Train PPL: 125.197\n",
      "\t Val. Loss: 7.379 |  Val. PPL: 1601.658\n",
      "Epoch: 13 | Time: 0m 8s\n",
      "\tTrain Loss: 4.811 | Train PPL: 122.897\n",
      "\t Val. Loss: 7.413 |  Val. PPL: 1657.613\n",
      "Epoch: 14 | Time: 0m 8s\n",
      "\tTrain Loss: 4.794 | Train PPL: 120.814\n",
      "\t Val. Loss: 7.443 |  Val. PPL: 1708.684\n",
      "Epoch: 15 | Time: 0m 8s\n",
      "\tTrain Loss: 4.776 | Train PPL: 118.579\n",
      "\t Val. Loss: 7.477 |  Val. PPL: 1767.251\n",
      "Epoch: 16 | Time: 0m 8s\n",
      "\tTrain Loss: 4.759 | Train PPL: 116.632\n",
      "\t Val. Loss: 7.521 |  Val. PPL: 1845.691\n",
      "Epoch: 17 | Time: 0m 8s\n",
      "\tTrain Loss: 4.745 | Train PPL: 115.020\n",
      "\t Val. Loss: 7.540 |  Val. PPL: 1881.330\n",
      "Epoch: 18 | Time: 0m 12s\n",
      "\tTrain Loss: 4.733 | Train PPL: 113.631\n",
      "\t Val. Loss: 7.566 |  Val. PPL: 1931.135\n",
      "Epoch: 19 | Time: 0m 10s\n",
      "\tTrain Loss: 4.719 | Train PPL: 112.096\n",
      "\t Val. Loss: 7.593 |  Val. PPL: 1983.354\n",
      "Epoch: 20 | Time: 0m 9s\n",
      "\tTrain Loss: 4.708 | Train PPL: 110.842\n",
      "\t Val. Loss: 7.620 |  Val. PPL: 2039.438\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epoch):    \n",
    "    start_time = time.time()\n",
    "    train_loss = train(seq2seq, trainloader, optimizer, criterion)\n",
    "    valid_loss = evaluate(seq2seq, devloader, criterion)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    # it's easier to see a change in perplexity between epoch as it's an exponential\n",
    "    # of the loss, hence the scale of the measure is much bigger\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-45410f8b4900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-bec0d0b19d5f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, text_batch, headline_batch, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# last hidden & cell state of the encoder is used as the decoder's initial hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-3441d31b031b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = seq2seq(test[:][0], test[:][1], teacher_forcing_ratio=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKUP (ALL CODES BELOW NOT USED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size):\n",
    "#         super(DecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, input, hidden):\n",
    "#         output = self.embedding(input).view(1, 1, -1)\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "#         output = self.softmax(self.out(output[0]))\n",
    "#         return output, hidden\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "# #         return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AttnDecoderRNN(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, dropout_p = 0.1, max_length = text_length_threshold):\n",
    "#         super(AttnDecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.dropout_p = dropout_p\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.dropout_p)\n",
    "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "#     def forward(self, input, hidden, encoder_outputs):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         embedded = self.dropout(embedded)\n",
    "\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "\n",
    "#         output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#         return output, hidden, attn_weights\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=text_length_threshold):\n",
    "#     encoder_hidden = encoder.initHidden()\n",
    "\n",
    "#     encoder_optimizer.zero_grad()\n",
    "#     decoder_optimizer.zero_grad()\n",
    "\n",
    "#     input_length = input_tensor.size(0)\n",
    "#     target_length = target_tensor.size(0)\n",
    "# #     input_length = len(input_tensor)\n",
    "# #     target_length = len(target_tensor)\n",
    "                       \n",
    "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "#     loss = 0\n",
    "#     #print('bbbbbbb-->>> input length', input_length)\n",
    "\n",
    "#     for ei in range(input_length):\n",
    "#         encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "#         #print(\"priting before error\")\n",
    "#         #print(encoder_output.size())\n",
    "#         #print(encoder_outputs.size())\n",
    "#         temp = encoder_output[0, 0]\n",
    "#         #print(temp)\n",
    "#         encoder_outputs[ei] = temp\n",
    "\n",
    "#     decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "#     decoder_hidden = encoder_hidden\n",
    "\n",
    "#     #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "#     #Without teacher forcing: use its own predictions as the next input\n",
    "\n",
    "#     #print('aaaaa-->>>')\n",
    "\n",
    "#     for di in range(target_length):\n",
    "#         decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "#         topv, topi = decoder_output.topk(1)\n",
    "      \n",
    "#         decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "      \n",
    "#         loss += criterion(decoder_output, target_tensor[di])\n",
    "      \n",
    "#         if decoder_input.item() == EOS_token:\n",
    "#             break\n",
    "\n",
    "#     loss.backward()\n",
    "\n",
    "#     encoder_optimizer.step()\n",
    "#     decoder_optimizer.step()\n",
    "\n",
    "#     return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "#     start = time.time()\n",
    "#     plot_losses = []\n",
    "#     print_loss_total = 0  # Reset every print_every\n",
    "#     plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "#     encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "#     decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "# #     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "# #                       for i in range(n_iters)]\n",
    "#     criterion = nn.NLLLoss()\n",
    "\n",
    "#     for i in range(1, n_iters + 1):\n",
    "# #         training_pair = training_pairs[iter - 1]\n",
    "# #         input_tensor = training_pair[0]\n",
    "# #         target_tensor = training_pair[1]\n",
    "#         input_tensor = text_train[i]\n",
    "#         target_tensor = headline_train[i]\n",
    "\n",
    "#         loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#         print_loss_total += loss\n",
    "#         plot_loss_total += loss\n",
    "\n",
    "#         if i % print_every == 0:\n",
    "#             print_loss_avg = print_loss_total / print_every\n",
    "#             print_loss_total = 0\n",
    "# #             print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "# #                                          iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "#         if i % plot_every == 0:\n",
    "#             plot_loss_avg = plot_loss_total / plot_every\n",
    "#             plot_losses.append(plot_loss_avg)\n",
    "#             plot_loss_total = 0\n",
    "\n",
    "#     showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-f1c7de392b64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadline_train_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainIters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-157-cacecf4f0265>\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheadline_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-134-2217520ac9ec>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mencoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m#print(\"priting before error\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#print(encoder_output.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-eea4319ec3a3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "# hidden_size = 256\n",
    "# encoder1 = EncoderRNN(len(text_train_str), hidden_size).to(device)\n",
    "# attn_decoder1 = AttnDecoderRNN(hidden_size, len(headline_train_str), dropout_p=0.1).to(device)\n",
    "\n",
    "# trainIters(encoder1, attn_decoder1, 150, print_every=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderRNN(nn.Module):\n",
    "\n",
    "#     def __init__(self, embed_size, hidden_size, bidi=True, *, rnn_drop: float=0):\n",
    "#         super(EncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_directions = 2 if bidi else 1\n",
    "#         self.gru = nn.GRU(embed_size, hidden_size, bidirectional=bidi, dropout=rnn_drop)\n",
    "\n",
    "#     def forward(self, embedded, hidden, input_lengths=None):\n",
    "#     \"\"\"\n",
    "#     :param embedded: (src seq len, batch size, embed size)\n",
    "#     :param hidden: (num directions, batch size, encoder hidden size)\n",
    "#     :param input_lengths: list containing the non-padded length of each sequence in this batch;\n",
    "#                           if set, we use `PackedSequence` to skip the PAD inputs and leave the\n",
    "#                           corresponding encoder states as zeros\n",
    "#     :return: (src seq len, batch size, hidden size * num directions = decoder hidden size)\n",
    "#     Perform multi-step encoding.\n",
    "#     \"\"\"\n",
    "#         if input_lengths is not None:\n",
    "#             embedded = pack_padded_sequence(embedded, input_lengths)\n",
    "\n",
    "#         output, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "#         if input_lengths is not None:\n",
    "#             output, _ = pad_packed_sequence(output)\n",
    "\n",
    "#         if self.num_directions > 1:\n",
    "#           # hidden: (num directions, batch, hidden) => (1, batch, hidden * 2)\n",
    "#             batch_size = hidden.size(1)\n",
    "#             hidden = hidden.transpose(0, 1).contiguous().view(1, batch_size, self.hidden_size * self.num_directions)\n",
    "#         return output, hidden\n",
    "\n",
    "#     def init_hidden(self, batch_size):\n",
    "#         return torch.zeros(self.num_directions, batch_size, self.hidden_size, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pytorch library for BERT\n",
    "# import torch\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# # Load pre-trained model tokenizer (vocabulary)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenized input\n",
    "# text = \"Who was Jim Henson ? Jim Henson was a puppeteer\"\n",
    "# tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "# masked_index = 6\n",
    "# tokenized_text[masked_index] = '[MASK]'\n",
    "# assert tokenized_text == ['who', 'was', 'jim', 'henson', '?', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer']\n",
    "\n",
    "# # Convert token to vocabulary indices\n",
    "# indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "# segments_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# # Convert inputs to PyTorch tensors\n",
    "# tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# segments_tensors = torch.tensor([segments_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import networkx as nx\n",
    "# from datetime import datetime as dt\n",
    "\n",
    "# t1 = dt.now()\n",
    "# print(t1)\n",
    "\n",
    "# output_file = 'train_stats_dict_processed_extr_final_5000_inc_pagerank.pickle' \n",
    "# input_file = 'train_stats_dict_processed_extr_final_5000_.pickle' \n",
    "# data = pd.read_pickle(input_file )\n",
    "\n",
    "# #Select sentence embeddings only and match to doc label\n",
    "# df_embed = data['df_X'].loc[:,'Sent_BERT_D_0': 'Sent_BERT_D_767']\n",
    "# df_doc_label = pd.DataFrame(data['Xy_doc_label_array'],columns=['doc_label'])\n",
    "# df = pd.concat([df_doc_label, df_embed], axis=1)\n",
    "\n",
    "# #loop through articles (docs)\n",
    "# pagerank_scores_list=[]\n",
    "# error_list = []\n",
    "# doc_num = np.max(data['Xy_doc_label_array']) \n",
    "# for j in range(doc_num+1):\n",
    "    \n",
    "#     #calculate cosine similiarity matrix \n",
    "#     df_doc = df [df.doc_label == j].iloc[:,2:]\n",
    "#     n = df_doc.shape[0]\n",
    "#     cos_matrix = cosine_similarity(df_doc, df_doc)\n",
    "#     f = np.vectorize(lambda x: 0 if x == 1 else 1)\n",
    "#     not_eye = f(np.eye(n,n))\n",
    "#     cos_matrix = cos_matrix * not_eye\n",
    "    \n",
    "#     #Convert to nx graph\n",
    "#     graph = nx.from_numpy_array(cos_matrix)\n",
    "    \n",
    "#     #Calculate sentence scores and record error docs\n",
    "#     try:\n",
    "#         scores_arr = np.array(list(nx.pagerank(graph, max_iter=500).values()))\n",
    "#     except:\n",
    "#         scores_arr = np.nan\n",
    "#         error_list.append(j)\n",
    "   \n",
    "#     pagerank_scores_list.append(scores_arr)\n",
    "    \n",
    "# pagerank_scores_arr = np.array(pagerank_scores_list)\n",
    "\n",
    "# #store in primary dictionary\n",
    "# data.update({'textrank_scores_arr_per_doc':pagerank_scores_arr })\n",
    "\n",
    "# #save to pickle\n",
    "# with open(output_file, 'wb') as handle:                                     \n",
    "#     pickle.dump(data, handle)\n",
    "\n",
    "# t2=dt.now()\n",
    "# print(t2)\n",
    "# print(t2-t1)\n",
    "\n",
    "# #runtime 4mins50sec for 5000 docs / 29 errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
