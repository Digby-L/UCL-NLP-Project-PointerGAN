{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from nltk import sent_tokenize\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adagrad\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Pytorch library for training\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# from torchtext.data import Field, BucketIterator, Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Dataset contains:\n",
    "##### data after padding: \n",
    "0) text_train_pad, 1) headline_train_pad, \n",
    "##### pad mask of data(1=padded, 0=not padded): \n",
    "2) text_train_padmask, 3) headline_train_padmask,\n",
    "##### sentence length: \n",
    "4) text_train_len, 5) headline_train_len\n",
    "##### out of vocabulary\n",
    "6) text_train_oov, 7) headline_train_oov\n",
    "#### dataset with unk embedding\n",
    "8) text_train_no, 9) headline_train_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_zip = torch.load('./Dataset7/traindata_zip.pt')\n",
    "\n",
    "devdata_zip = torch.load('./Dataset7/devdata_zip.pt')\n",
    "\n",
    "testdata_zip = torch.load('./Dataset7/testdata_zip.pt')\n",
    "\n",
    "embedding = np.load('./Dataset7/embedding.npy')\n",
    "\n",
    "embedding_headline = np.load('./Dataset7/embedding_headline.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    PAD_token = 0   # Used for padding short sentences\n",
    "    SOS_token = 1   # Start-of-sentence token\n",
    "    EOS_token = 2   # End-of-sentence token\n",
    "    UNK_token = 3   # Out-of-vocabulary token\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"pad\":0, \"sos\":1, \"eos\":2, \"unk\":3}\n",
    "        self.word2count = {\"pad\":0, \"sos\":0, \"eos\":0, \"unk\":0}              \n",
    "        self.index2word = {0: \"pad\", 1: \"sos\", 2: \"eos\", 3: \"unk\"}\n",
    "        self.num_words = 4\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        for word in sentence:           \n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            self.longest_sentence = sentence_len\n",
    "        self.num_sentences += 1\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_dictionary\n",
    "with open('./Dataset7/text_vocabulary.pgn', 'rb') as text_dictionary_file: #for S2S and S2S+GAN\n",
    "    text_vocabulary = pickle.load(text_dictionary_file)\n",
    "# headline_dictionary\n",
    "with open('./Dataset7/headline_vocabulary.pgn', 'rb') as headline_dictionary_file: #for S2S and S2S+GAN\n",
    "    headline_vocabulary = pickle.load(headline_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set batch size and split data after padding to batches\n",
    "def batch_dataloader(data, Batch_size):\n",
    "    data_dataloader = torch.utils.data.DataLoader(data, batch_size=Batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return data_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data batching\n",
    "trainloader = batch_dataloader(traindata_zip, 100)\n",
    "\n",
    "devloader = batch_dataloader(devdata_zip, 20)\n",
    "\n",
    "testloader = batch_dataloader(testdata_zip, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare dataset for training\n",
    "text_testrun, hl_testrun, text_train_padmask, headline_train_padmask, text_train_len, \\\n",
    "headline_train_len, text_train_oov, headline_train_oov, text_train_no, headline_train_no= next(iter(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If need to send to cuda\n",
    "text_testrun = text_testrun.to(device)\n",
    "hl_testrun = hl_testrun.to(device)\n",
    "\n",
    "text_train_padmask = text_train_padmask.to(device)\n",
    "headline_train_padmask = headline_train_padmask.to(device)\n",
    "\n",
    "text_train_oov = text_train_oov.to(device)\n",
    "headline_train_oov = headline_train_oov.to(device)\n",
    "\n",
    "text_train_no = text_train_no.to(device)\n",
    "headline_train_no = headline_train_no.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for test run ------ CAN DELETE THIS CELL IN FUTURE\n",
    "input_size = int(len(text_vocabulary.index2word.keys())+1)\n",
    "output_size = int(len(headline_vocabulary.index2word.keys())+1)\n",
    "\n",
    "vocab_size = input_size - 1\n",
    "unk_idx = 3\n",
    "use_coverage = True\n",
    "use_p_gen = True\n",
    "\n",
    "enc_emb_size = 200\n",
    "dec_emb_size = 200\n",
    "hid_size = 128\n",
    "\n",
    "n_layers = 1\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "beam_size = 50\n",
    "max_dec_steps = 10\n",
    "min_dec_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Generator Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hid_size, emb_size, embedding):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_size, hid_size, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.W_h = nn.Linear(hid_size * 2, hid_size * 2, bias=False)\n",
    "\n",
    "    #seq_lens should be in descending order\n",
    "    def forward(self, enc_input, enc_input_len, hidden_state):\n",
    "        embedded = torch.tensor([[self.embedding[i] for i in enc_input[:, seq]] \\\n",
    "                                 for seq in range(enc_input.shape[1])]).permute(1, 0, 2)\n",
    "\n",
    "        packed = pack_padded_sequence(embedded, enc_input_len, batch_first=True, enforce_sorted= False)\n",
    "            \n",
    "        output, hidden = self.lstm(packed, hidden_state)\n",
    "        \n",
    "        enc_outputs, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        enc_outputs = enc_outputs.contiguous()\n",
    "        \n",
    "        enc_feature = enc_outputs.view(-1, 2*hid_size)  # B * L x 2*hidden_dim\n",
    "        enc_feature = self.W_h(enc_feature)\n",
    "\n",
    "        return enc_outputs, enc_feature, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 100, 128]),\n",
       " torch.Size([2, 100, 128]),\n",
       " torch.Size([100, 174, 256]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testrun\n",
    "# Call Encoder\n",
    "encoder = Encoder(input_size, hid_size, enc_emb_size, embedding).to(device)\n",
    "\n",
    "text_train_len = torch.sum(torch.where(text_testrun==0,0,1), axis = 1).tolist()\n",
    "\n",
    "# Check output of Encoder\n",
    "enc_outputs, enc_feature, final_hidden = encoder(text_testrun.to(device), text_train_len, None)\n",
    "\n",
    "final_hidden[0].shape, final_hidden[1].shape, enc_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimension of Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceState(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.reduce_h = nn.Linear(hid_size * 2, hid_size)\n",
    "        self.reduce_c = nn.Linear(hid_size * 2, hid_size)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        h, c = hidden\n",
    "        \n",
    "        h_in = h.transpose(0, 1).contiguous().view(-1, hid_size * 2)\n",
    "        hid_reduced_h = F.relu(self.reduce_h(h_in))\n",
    "        \n",
    "        c_in = c.transpose(0, 1).contiguous().view(-1, hid_size * 2)\n",
    "        hid_reduced_c = F.relu(self.reduce_c(c_in))\n",
    "\n",
    "        return hid_reduced_h.unsqueeze(0), hid_reduced_c.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 128]), torch.Size([1, 100, 128]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run\n",
    "reduce = ReduceState().to(device)\n",
    "reduce_hidden = reduce(final_hidden)\n",
    "reduce_hidden[0].shape, reduce_hidden[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_size, use_coverage):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_coverage = use_coverage\n",
    "        \n",
    "        # Coverage layer\n",
    "        if use_coverage:\n",
    "            self.w_c = nn.Linear(1, hid_size * 2, bias=False)\n",
    "            \n",
    "        self.decode_proj = nn.Linear(hid_size * 2, hid_size * 2)\n",
    "        self.v = nn.Linear(hid_size * 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, h_c_hat, enc_outputs, enc_feature, enc_padding_mask, coverage):\n",
    "        \"\"\"\"\"\n",
    "        h_c_hat: hidden, cell from decoder\n",
    "        enc_outputs: first output of encoder\n",
    "        enc_feature: second output of encoder\n",
    "        enc_padding_mask: text_padmask\n",
    "        coverage: initialize: Variable(torch.zeros((batch_size, 2 * hid_size)))\n",
    "        \n",
    "        Return:\n",
    "        context_vec: sum(attention weights)*encoder hidden states\n",
    "        attn_dist: attention distribution\n",
    "        coverage: updated coverage\n",
    "        \"\"\"\"\"\n",
    "        b, m, n = list(enc_outputs.size())\n",
    "\n",
    "        dec_feature = self.decode_proj(h_c_hat) # B x 2*hid_size\n",
    "        \n",
    "        dec_feature_expanded = dec_feature.unsqueeze(1).expand(b, m, n).contiguous() # B x m x 2*hid_size\n",
    "        \n",
    "        dec_feature_expanded = dec_feature_expanded.view(-1, n)  # (B * m )x 2*hid_size\n",
    "\n",
    "        attn_feature = enc_feature + dec_feature_expanded # (B * m) x 2*hid_size\n",
    "        \n",
    "        if self.use_coverage:\n",
    "            coverage_input = coverage.view(-1, 1)  # (B * m) x 1\n",
    "            coverage_feature = self.w_c(coverage_input)  # (B * m) x 2*hid_size\n",
    "            att_feature = attn_feature + coverage_feature\n",
    "\n",
    "        scores = torch.tanh(att_feature) # (B * m) x 2*hidden_dim\n",
    "        scores = self.v(scores)  # (B * m) x 1\n",
    "        scores = scores.view(-1, m)  # B x m\n",
    "\n",
    "        attn_dist = F.softmax(scores, dim=1) * (1-enc_padding_mask) # B x m\n",
    "        normalization_factor = attn_dist.sum(1, keepdim = True)\n",
    "        \n",
    "        attn_dist = attn_dist / normalization_factor\n",
    "\n",
    "        attn_dist = attn_dist.unsqueeze(1)  # B x 1 x m\n",
    "        context_vec = torch.bmm(attn_dist, enc_outputs)  # B x 1 x n\n",
    "        context_vec = context_vec.view(-1, hid_size * 2)  # B x 2*hidden_dim\n",
    "\n",
    "        attn_dist = attn_dist.view(-1, m)  # B x m\n",
    "\n",
    "        if self.use_coverage:\n",
    "            coverage = coverage.view(-1, m)\n",
    "            coverage = coverage + attn_dist\n",
    "\n",
    "        return context_vec, attn_dist, coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run\n",
    "attn_net = Attention(hid_size, True).to(device)\n",
    "\n",
    "# Set inputs to attn_net\n",
    "coverage = Variable(torch.zeros(text_testrun.size())).to(device)\n",
    "\n",
    "h_decoder, c_decoder = reduce_hidden\n",
    "h_c_hat = torch.cat((h_decoder.view(-1, hid_size), c_decoder.view(-1, hid_size)), 1).to(device)\n",
    "\n",
    "# Check outputs of Attention class\n",
    "context_vec, attn_dist, coverage = attn_net(h_c_hat, enc_outputs, enc_feature, text_train_padmask.to(device), coverage=coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Generator Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hid_size, vocab_size, emb_size, embedding_headline, use_coverage, use_p_gen):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_net = Attention(hid_size, use_coverage)\n",
    "        \n",
    "        self.use_coverage = use_coverage   #True/False\n",
    "        \n",
    "        self.use_p_gen = use_p_gen  #True/False\n",
    "\n",
    "        self.embedding = embedding_headline \n",
    "\n",
    "        self.x_context = nn.Linear(hid_size * 2 + emb_size, emb_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_size, hid_size, num_layers=1, batch_first=True, bidirectional=False)\n",
    "\n",
    "        if use_p_gen:\n",
    "            self.p_gen_linear = nn.Linear(hid_size * 4 + emb_size, 1)\n",
    "\n",
    "        #p_vocab\n",
    "        self.out1 = nn.Linear(hid_size * 3, hid_size)\n",
    "        self.out2 = nn.Linear(hid_size, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, target, h_c_1, enc_outputs, enc_feature, enc_padding_mask,\n",
    "                cont_v, enc_oov_len, enc_batch, coverage, step):\n",
    "        \"\"\"\n",
    "        target: headline batch\n",
    "        h_c_1: reduced_state(enc_hidden)\n",
    "        h_c_hat: updated hidden for attn_net\n",
    "        enc_outputs: first output of encoder\n",
    "        enc_feature: second output of encoder\n",
    "        enc_padding_mask: \n",
    "        cont_v: context vector input to decoder (initialization: Variable(torch.zeros((batch_size, 2 * hid_size))))\n",
    "        enc_oov_len: text_batch_oov\n",
    "        enc_batch: text_train_pad (OOV has index)\n",
    "        coverage: initialization: Variable(torch.zeros(text_batch.size()))\n",
    "        \n",
    "        extro_zeros: initialization: Variable(torch.zeros((batch_size, max_oov_len)))\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.training and step == 0:\n",
    "            h_decoder, c_decoder = h_c_1\n",
    "            h_c_hat = torch.cat((h_decoder.view(-1, hid_size),\n",
    "                                 c_decoder.view(-1, hid_size)), 1)  # B x 2*hid_size\n",
    "            context_vec, _, coverage_new = self.attn_net(h_c_hat, enc_outputs, enc_feature,\n",
    "                                                  enc_padding_mask, coverage)\n",
    "            coverage = coverage_new\n",
    "        \n",
    "        max_oov_len = torch.max(enc_oov_len)\n",
    "        \n",
    "        target_embbed = torch.tensor([self.embedding[i] for i in target]).float()\n",
    "        \n",
    "        x = self.x_context(torch.cat((cont_v, target_embbed), 1))\n",
    "        lstm_out, h_c = self.lstm(x.unsqueeze(1), h_c_1)\n",
    "\n",
    "        h_decoder, c_decoder = h_c\n",
    "        h_c_hat = torch.cat((h_decoder.view(-1, hid_size),\n",
    "                             c_decoder.view(-1, hid_size)), 1)  # B x 2*hid_size\n",
    "        context_vec, attn_dist, coverage_new = self.attn_net(h_c_hat, enc_outputs, enc_feature,\n",
    "                                                          enc_padding_mask, coverage)\n",
    "\n",
    "        if self.training or step > 0:\n",
    "            coverage = coverage_new\n",
    "\n",
    "        p_gen = None\n",
    "        \n",
    "        if self.use_p_gen:\n",
    "            p_gen_input = torch.cat((cont_v, h_c_hat, x), 1)  # B x (2*2*hid_size + emb_size)\n",
    "            p_gen = self.p_gen_linear(p_gen_input)\n",
    "            p_gen = torch.sigmoid(p_gen)\n",
    "\n",
    "        output = torch.cat((lstm_out.view(-1, hid_size), cont_v), 1) # B x hid_size * 3\n",
    "        output = self.out1(output) # B x hid_size\n",
    "\n",
    "        output = self.out2(output) # B x vocab_size\n",
    "        vocab_dist = F.softmax(output, dim=1)\n",
    "        \n",
    "        dist_size = vocab_dist.size(0)\n",
    "        extra_zeros = torch.zeros((dist_size, max_oov_len), device=vocab_dist.device)\n",
    "\n",
    "        if self.use_p_gen:\n",
    "            vocab_dist_p = p_gen * vocab_dist\n",
    "            attn_dist_p = (1 - p_gen) * attn_dist\n",
    "\n",
    "            if extra_zeros is not None:\n",
    "                vocab_dist_p = torch.cat([vocab_dist_p, extra_zeros], 1)\n",
    "\n",
    "            final_dist = vocab_dist_p.scatter_add(1, enc_batch, attn_dist_p)\n",
    "        else:\n",
    "            final_dist = vocab_dist\n",
    "\n",
    "        return final_dist, h_c, cont_v, attn_dist, p_gen, coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run\n",
    "decoder = Decoder(input_size, hid_size, vocab_size, dec_emb_size, embedding_headline, use_coverage=True, use_p_gen=True).to(device)\n",
    "\n",
    "\n",
    "# Set inputs to decoder\n",
    "cont_v =  Variable(torch.zeros((100, 2 * hid_size))).to(device)\n",
    "\n",
    "max_oov_len = torch.max(text_train_oov).to(device)\n",
    "h_c_1 = reduce_hidden\n",
    "\n",
    "extra_zeros = Variable(torch.zeros((100, max_oov_len))).to(device)\n",
    "\n",
    "# Check outputs from Decoder class\n",
    "final_dist, h_c, cont_v, attn_dist, p_gen, coverage = decoder(hl_testrun[:,0].to(device), h_c_1, enc_outputs, enc_feature, \n",
    "                                                              text_train_padmask.to(device), cont_v, text_train_oov.to(device), \n",
    "                                                              text_testrun.to(device), coverage, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    ## Set parameter\n",
    "    input_size = int(len(text_vocabulary.index2word.keys())+1)\n",
    "    output_size = int(len(headline_vocabulary.index2word.keys())+1)\n",
    "\n",
    "    vocab_size = input_size - 1\n",
    "    unk_idx = 3\n",
    "    use_coverage = True\n",
    "    use_p_gen = True\n",
    "\n",
    "    enc_emb_size = 200\n",
    "    dec_emb_size = 200\n",
    "    hid_size = 128\n",
    "\n",
    "    n_layers = 1\n",
    "    enc_dropout = 0.5\n",
    "    dec_dropout = 0.5\n",
    "\n",
    "    beam_size = 50\n",
    "    max_dec_steps = 10\n",
    "    min_dec_steps = 1\n",
    "    \n",
    "    cov_weight = 1.0\n",
    "    \n",
    "    # training and optimizer\n",
    "    lr = 0.1\n",
    "    opt_acc = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer_Generator(nn.Module):\n",
    "    def __init__(self, para, encoder: Encoder, reduced_net: ReduceState, decoder: Decoder, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.reduced_net = reduced_net\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.para = para\n",
    "        \n",
    "\n",
    "    def forward(self, text_batch, text_batch_len, text_batch_padmask, text_batch_oov, \\\n",
    "                headline_batch, headline_batch_len, headline_batch_no, headline_batch_padmask, hidden_state):\n",
    "        \"\"\"\"\"\n",
    "        text_batch: text batch with oov index (eg. text_train_pad)\n",
    "        text_batch_len: len of sentence in the batch before padding (eg. text_train_len)\n",
    "        \n",
    "        text_batch_padmask: padding mask of each sentence in text batch (padded:1, no pad: 0), (eg. text_train_padmask)\n",
    "        text_batch_oov: number of oov in each sentence in text batch (eg. text_train_oov)\n",
    "\n",
    "        headline_batch: headline batch with oov index (eg. headline_train_pad)\n",
    "        headline_batch_len: len of sentence in the batch before padding (eg. headline_train_len)\n",
    "        \n",
    "        headline_batch_no: headline batch with oov index == unk_idx == 3 (eg. headline_train_no)\n",
    "        headline_batch_padmask: padding mask of each sentence in hl batch (padded:1, no pad: 0), (eg. headline_train_padmask)\n",
    "        \n",
    "        hidden_state: hidden state for GAN (hidden_state = None, if not specified)\n",
    "        \"\"\"\"\"\n",
    "        \n",
    "        batch_size, max_len = headline_batch.shape\n",
    "        headline_vocab_size = self.para.output_size - 1\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, headline_vocab_size)\n",
    "\n",
    "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "        enc_outputs, enc_feature, enc_hidden = self.encoder(text_batch, text_batch_len, hidden_state)\n",
    "        \n",
    "        h_c_1 = self.reduced_net(enc_hidden)\n",
    "        \n",
    "        c_t_1 = Variable(torch.zeros((100, 2 * self.para.hid_size)))\n",
    "        \n",
    "        coverage = Variable(torch.zeros((batch_size, max(text_batch_len))))\n",
    "        print(coverage.shape)\n",
    "#         coverage = Variable(torch.zeros(text_batch.size()))\n",
    "        \n",
    "        dist = torch.zeros((batch_size, max_len, self.para.vocab_size))\n",
    "        \n",
    "        step_loss_rec = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            final_dist, h_c, c_t, attn_dist, p_gen, coverage_new = self.decoder(headline_batch_no[:,i], h_c_1, enc_outputs, enc_feature, \n",
    "                                                                  text_batch_padmask, c_t_1, text_batch_oov, \n",
    "                                                                  text_batch, coverage, i)\n",
    "            # Record distribution for GAN\n",
    "            dist[:, i, :] = final_dist\n",
    "            \n",
    "            \n",
    "            # Calculate loss for this batch\n",
    "            headline = headline_batch[:, i]\n",
    "            total_dist = torch.gather(final_dist, 1, headline.unsqueeze(1)).squeeze()\n",
    "            step_loss = - torch.log(total_dist + 1e-10)\n",
    "            \n",
    "            if self.para.use_coverage:\n",
    "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "                \n",
    "                step_loss = step_loss + step_coverage_loss\n",
    "                coverage = coverage_new\n",
    "                \n",
    "            step_mask = headline_batch_padmask[:, i]\n",
    "            step_loss = step_loss + 1.0*step_mask\n",
    "            step_loss_rec.append(step_loss)\n",
    "\n",
    "        total_loss = torch.sum(torch.stack(step_loss_rec, 1), 1)\n",
    "        \n",
    "        batch_loss_average = total_loss/headline_batch_len\n",
    "        \n",
    "        # Final loss is the average of batch_loss_average\n",
    "        loss = torch.mean(batch_loss_average)\n",
    "\n",
    "        return dist, loss           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 174])\n"
     ]
    }
   ],
   "source": [
    "# Test run\n",
    "para = Parameters()\n",
    "encoder = Encoder(para.input_size, para.hid_size, para.enc_emb_size, embedding).to(device)\n",
    "reduce = ReduceState().to(device)\n",
    "decoder = Decoder(para.input_size, para.hid_size, para.vocab_size, para.dec_emb_size, embedding_headline, \\\n",
    "                  para.use_coverage, para.use_p_gen).to(device)\n",
    "\n",
    "PG = Pointer_Generator(para, encoder, reduce, decoder, device).to(device)\n",
    "dist, loss = PG(text_testrun.to(device), text_train_len, text_train_padmask.to(device), text_train_oov.to(device), \\\n",
    "          hl_testrun.to(device), headline_train_len, headline_train_no.to(device), \\\n",
    "                headline_train_padmask.to(device), hidden_state = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99.4610, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain pointer generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = Parameters()\n",
    "encoder = Encoder(para.input_size, para.hid_size, para.enc_emb_size, embedding).to(device)\n",
    "reduce = ReduceState().to(device)\n",
    "decoder = Decoder(para.input_size, para.hid_size, para.vocab_size, para.dec_emb_size, embedding_headline, \\\n",
    "                  para.use_coverage, para.use_p_gen).to(device)\n",
    "\n",
    "model_params = list(encoder.parameters()) + list(decoder.parameters()) + list(reduce.parameters())\n",
    "\n",
    "optimizer = Adagrad(model_params, lr = para.lr, initial_accumulator_value = para.opt_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pretrain_PGN(PG: Pointer_Generator, dataloader, optimizer, hidden_state, loss_decay = 0.9):\n",
    "    \n",
    "    PG.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for text_batch, hl_batch, text_batch_padmask, headline_batch_padmask, text_batch_len, \\\n",
    "    headline_batch_len, text_batch_oov, headline_batch_oov, \\\n",
    "    text_batch_no, headline_batch_no in dataloader:\n",
    "\n",
    "        ## send to cuda\n",
    "        text_batch = text_batch.to(device)\n",
    "        hl_batch = hl_batch.to(device)\n",
    "        \n",
    "        text_batch_padmask = text_batch_padmask.to(device)\n",
    "        headline_batch_padmask = headline_batch_padmask.to(device)\n",
    "        \n",
    "        text_batch_len = text_batch_len.to(device)\n",
    "        headline_batch_len = headline_batch_len.to(device)\n",
    "        \n",
    "        text_batch_oov = text_batch_oov.to(device)\n",
    "        headline_batch_oov = headline_batch_oov.to(device)\n",
    "        \n",
    "        text_batch_no = text_batch_no.to(device)\n",
    "        headline_batch_no = headline_batch_no.to(device)\n",
    "        \n",
    "        _, loss = PG(text_batch, text_batch_len, text_batch_padmask, text_batch_oov, \\\n",
    "          hl_batch, headline_batch_len, headline_batch_no, headline_batch_padmask, hidden_state)\n",
    "\n",
    "#         if epoch_loss == 0:\n",
    "#             epoch_loss = loss\n",
    "        \n",
    "#         else:\n",
    "#             epoch_loss = epoch_loss * loss_decay + (1 - loss_decay) * loss \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "        if epoch_loss == 0:\n",
    "            epoch_loss = loss.item()\n",
    "        \n",
    "        else:\n",
    "            epoch_loss = epoch_loss * loss_decay + (1 - loss_decay) * loss.item()\n",
    "   \n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pretrain_evaluation(PG: Pointer_Generator, dataloader, hidden_state, loss_decay = 0.9):\n",
    "    \n",
    "    PG.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, hl_batch, text_batch_padmask, headline_batch_padmask, text_batch_len, \\\n",
    "        headline_batch_len, text_batch_oov, headline_batch_oov, \\\n",
    "        text_batch_no, headline_batch_no in dataloader:\n",
    "\n",
    "            ## send to cuda\n",
    "            text_batch = text_batch.to(device)\n",
    "            hl_batch = hl_batch.to(device)\n",
    "\n",
    "            text_batch_padmask = text_batch_padmask.to(device)\n",
    "            headline_batch_padmask = headline_batch_padmask.to(device)\n",
    "\n",
    "            text_batch_len = text_batch_len.to(device)\n",
    "            headline_batch_len = headline_batch_len.to(device)\n",
    "\n",
    "            text_batch_oov = text_batch_oov.to(device)\n",
    "            headline_batch_oov = headline_batch_oov.to(device)\n",
    "\n",
    "            text_batch_no = text_batch_no.to(device)\n",
    "            headline_batch_no = headline_batch_no.to(device)\n",
    "\n",
    "            _, loss = PG(text_batch, text_batch_len, text_batch_padmask, text_batch_oov, \\\n",
    "            hl_batch, headline_batch_len, headline_batch_no, headline_batch_padmask, hidden_state)\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "        if epoch_loss == 0:\n",
    "            epoch_loss = loss.item()\n",
    "        \n",
    "        else:\n",
    "            epoch_loss = epoch_loss * loss_decay + (1 - loss_decay) * loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search to output index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam(object):\n",
    "    def __init__(self, tokens, log_probs, state, context, coverage):\n",
    "        self.tokens = tokens\n",
    "        self.log_probs = log_probs\n",
    "        self.state = state\n",
    "        self.context = context\n",
    "        self.coverage = coverage\n",
    "\n",
    "    def extend(self, token, log_prob, state, context, coverage):\n",
    "        return Beam(tokens = self.tokens + [token],\n",
    "                          log_probs = self.log_probs + [log_prob],\n",
    "                          state = state,\n",
    "                          context = context,\n",
    "                          coverage = coverage)\n",
    "\n",
    "    @property\n",
    "    def latest_token(self):\n",
    "        return self.tokens[-1]\n",
    "\n",
    "    @property\n",
    "    def avg_log_prob(self):\n",
    "        return sum(self.log_probs) / len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam_Search(object):\n",
    "    def __init__(self, para: Parameters, encoder: Encoder, reduced_net: ReduceState, decoder: Decoder, device: torch.device):\n",
    "        \n",
    "        \n",
    "        self.para = Parameters()\n",
    "        self.unk_idx = 3\n",
    "        \n",
    "        # Call Encoder, ReducedState and Decoder\n",
    "        self.encoder = encoder\n",
    "        self.reduced_net = reduced_net\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def sort_beams(self, beams):\n",
    "        return sorted(beams, key=lambda h: h.avg_log_prob, reverse=True)\n",
    "\n",
    "    \n",
    "    def beam_search(self, text, text_len, text_padmask, text_oov, beam_batch, hidden_state):\n",
    "        \"\"\"\"\"\n",
    "        text: one text data from the batch (eg. text_train_pad[1])\n",
    "        text_len: length of text (witout padding) (eg. text_train_len[1])\n",
    "        text_padmask: padding mask of the text, padded position = 1, not padded = 0 (eg. text_train_padmask[1])\n",
    "        text_oov: number of oov in a text (eg. text_train_oov[1])\n",
    "        \n",
    "        hidden_state: hidden state for discriminator (=None, if not specified)\n",
    "        \"\"\"\"\"\n",
    "        ## Reapeat this text to form a batch containing this text only\n",
    "        text_batch = torch.transpose(text.unsqueeze(1).repeat(1, beam_batch),0,1)\n",
    "        \n",
    "        text_len = torch.sum(torch.where(text==0,0,1)).tolist()\n",
    "        \n",
    "        text_batch_len = [text_len] * beam_batch\n",
    "        \n",
    "        text_batch_padmask = torch.transpose(text_padmask.unsqueeze(1).repeat(1, beam_batch),0,1)\n",
    "        \n",
    "        text_batch_oov =  torch.tensor([text_oov] * beam_batch).unsqueeze(1)\n",
    "        \n",
    "        # Initialize c_t and coverage\n",
    "        c_t_0 = Variable(torch.zeros((100, 2 * self.para.hid_size)))\n",
    "        coverage_t_0 = Variable(torch.zeros(text_batch.size()))\n",
    "        \n",
    "        \n",
    "        # Call encoder\n",
    "        enc_outputs, enc_feature, enc_hidden = self.encoder(text_batch, text_batch_len, hidden_state)\n",
    "        \n",
    "        h_c_0 = self.reduced_net(enc_hidden)\n",
    "\n",
    "        dec_h, dec_c = h_c_0 # 1 x 2*hidden_size\n",
    "        dec_h = dec_h.squeeze()\n",
    "        dec_c = dec_c.squeeze()\n",
    "\n",
    "        # Beam hypothesis\n",
    "        beams = [Beam(tokens=[1],\n",
    "                      log_probs=[0.0],\n",
    "                      state=(dec_h[0], dec_c[0]),\n",
    "                      context = c_t_0[0],\n",
    "                      coverage=(coverage_t_0[0])) for _ in range(beam_batch)]\n",
    "        \n",
    "        results = []\n",
    "        steps = 0\n",
    "        while steps < self.para.max_dec_steps and len(results) < self.para.beam_size:\n",
    "            latest_tokens = [h.latest_token for h in beams]\n",
    "            latest_tokens = [t if t < self.para.vocab_size else self.unk_idx for t in latest_tokens]\n",
    "            target = Variable(torch.LongTensor(latest_tokens))\n",
    "\n",
    "            all_state_h =[]\n",
    "            all_state_c = []\n",
    "\n",
    "            all_context = []\n",
    "\n",
    "            for h in beams:\n",
    "                state_h, state_c = h.state\n",
    "                all_state_h.append(state_h)\n",
    "                all_state_c.append(state_c)\n",
    "\n",
    "                all_context.append(h.context)\n",
    "\n",
    "            h_c_1 = (torch.stack(all_state_h, 0).unsqueeze(0), torch.stack(all_state_c, 0).unsqueeze(0))\n",
    "            c_t_1 = torch.stack(all_context, 0)\n",
    " \n",
    "            coverage_t_1 = None\n",
    "            if use_coverage:\n",
    "                all_coverage = []\n",
    "                for h in beams:\n",
    "                    all_coverage.append(h.coverage)\n",
    "                coverage_t_1 = torch.stack(all_coverage, 0)\n",
    "            \n",
    "            final_dist, h_c, c_t, attn_dist, p_gen, coverage_t = self.decoder(target, h_c_1, enc_outputs, enc_feature, \n",
    "                                                                  text_batch_padmask, c_t_1, text_batch_oov, \n",
    "                                                                  text_batch, coverage_t_1, steps)\n",
    "            \n",
    "            log_probs = torch.log(final_dist)\n",
    "            topk_log_probs, topk_ids = torch.topk(log_probs, self.para.beam_size*2)\n",
    "\n",
    "            dec_h, dec_c = h_c\n",
    "            dec_h = dec_h.squeeze()\n",
    "            dec_c = dec_c.squeeze()\n",
    "\n",
    "            all_beams = []\n",
    "\n",
    "            if steps == 0:\n",
    "                num_orig_beams = 1\n",
    "            else:\n",
    "                num_orig_beams = len(beams)\n",
    "                \n",
    "            for i in range(num_orig_beams):\n",
    "                h = beams[i]\n",
    "                state_i = (dec_h[i], dec_c[i])\n",
    "                context_i = c_t[i]\n",
    "                coverage_i = (coverage_t[i])\n",
    "\n",
    "                for j in range(self.para.beam_size * 2):  # for each of the top 2*beam_size hyps:\n",
    "                    new_beam = h.extend(token=topk_ids[i, j].item(),\n",
    "                                   log_prob=topk_log_probs[i, j].item(),\n",
    "                                   state=state_i,\n",
    "                                   context=context_i,\n",
    "                                   coverage=coverage_i)\n",
    "                    all_beams.append(new_beam)\n",
    "\n",
    "            beams = []\n",
    "            for h in self.sort_beams(all_beams):\n",
    "                if h.latest_token == [2]:\n",
    "                    if steps >= self.para.min_dec_steps:\n",
    "                        results.append(h)\n",
    "                else:\n",
    "                    beams.append(h)\n",
    "                if len(beams) == beam_batch or len(results) == self.para.beam_size:\n",
    "                    break\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "        if len(results) == 0:\n",
    "            results = beams\n",
    "\n",
    "        beams_sorted = self.sort_beams(results)\n",
    "\n",
    "        return beams_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (585) must match the size of tensor b (591) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-312-93e225185704>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Check result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_testrun\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_train_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_train_padmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_train_oov\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-311-4e9e6213f67a>\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, text, text_len, text_padmask, text_oov, beam_batch, hidden_state)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mcoverage_t_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_coverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             final_dist, h_c, c_t, attn_dist, p_gen, coverage_t = self.decoder(target, h_c_1, enc_outputs, enc_feature, \n\u001b[0m\u001b[0;32m     88\u001b[0m                                                                   \u001b[0mtext_batch_padmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_t_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_batch_oov\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                                                                   text_batch, coverage_t_1, steps)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-274-2c0a6dd4e383>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, target, h_c_1, enc_outputs, enc_feature, enc_padding_mask, cont_v, enc_oov_len, enc_batch, coverage, step)\u001b[0m\n\u001b[0;32m     57\u001b[0m         h_c_hat = torch.cat((h_decoder.view(-1, hid_size),\n\u001b[0;32m     58\u001b[0m                              c_decoder.view(-1, hid_size)), 1)  # B x 2*hid_size\n\u001b[1;32m---> 59\u001b[1;33m         context_vec, attn_dist, coverage_new = self.attn_net(h_c_hat, enc_outputs, enc_feature,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                                           enc_padding_mask, coverage)\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-272-ea58001171db>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, h_c_hat, enc_outputs, enc_feature, enc_padding_mask, coverage)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mcoverage_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoverage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (B * m) x 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mcoverage_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoverage_input\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (B * m) x 2*hid_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0matt_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_feature\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcoverage_feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matt_feature\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B * m) x 2*hidden_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (585) must match the size of tensor b (591) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Test run\n",
    "para = Parameters()\n",
    "encoder = Encoder(para.input_size, para.hid_size, para.enc_emb_size).to(device)\n",
    "reduce = ReduceState()\n",
    "decoder = Decoder(para.input_size, para.hid_size, para.vocab_size, para.dec_emb_size, \\\n",
    "                  para.use_coverage, para.use_p_gen)\n",
    "BS = Beam_Search(para, encoder, reduce, decoder, device)\n",
    "\n",
    "# Check result\n",
    "result = BS.beam_search(text_testrun[1], text_train_len[1], text_train_padmask[1], text_train_oov[1], beam_batch=3, hidden_state = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 12, 12, 8, 24, 11]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
