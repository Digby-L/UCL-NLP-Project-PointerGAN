{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "capital-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215365 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# read data from the csv file (from the location it is stored)\n",
    "Data = pd.read_csv('.\\Dataset\\wikihowAll.csv')\n",
    "\n",
    "Data = Data.astype(str)\n",
    "\n",
    "rows, columns = Data.shape\n",
    "print(rows, columns)\n",
    "\n",
    "# create a file to record the file names. This can be later used to divide the dataset in train/dev/test sets\n",
    "title_file = open('titles.txt', 'wb')\n",
    "\n",
    "# The path where the articles are to be saved\n",
    "path = \"articles\"\n",
    "if not os.path.exists(path): os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sublime-hepatitis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# go over the all the articles in the data file\n",
    "for row in range(rows):\n",
    "    abstract = Data.loc[row,'headline']      # headline is the column representing the summary sentences\n",
    "    article = Data.loc[row,'text']           # text is the column representing the article\n",
    "\n",
    "    #  a threshold is used to remove short articles with long summaries as well as articles with no summary\n",
    "    if len(abstract) < (0.75*len(article)):\n",
    "        # remove extra commas in abstracts\n",
    "        abstract = abstract.replace(\".,\",\".\")\n",
    "        abstract = abstract.encode('utf-8')\n",
    "        # remove extra commas in articles\n",
    "        article = re.sub(r'[.]+[\\n]+[,]',\".\\n\", article)\n",
    "        article = article.encode('utf-8')\n",
    "        \n",
    "\n",
    "        # a temporary file is created to initially write the summary, it is later used to separate the sentences of the summary\n",
    "        with open('temporaryFile.txt','wb') as t:\n",
    "            t.write(abstract)\n",
    "        \n",
    "        # file names are created using the alphanumeric charachters from the article titles.\n",
    "        # they are stored in a separate text file.\n",
    "        filename = Data.loc[row,'title']\n",
    "        filename = \"\".join(x for x in filename if x.isalnum())\n",
    "        filename1 = filename + '.txt'\n",
    "        filename = filename.encode('utf-8')\n",
    "        title_file.write(filename+b'\\n')\n",
    "        \n",
    "        \n",
    "\n",
    "        with open(path+'/'+filename1, 'wb') as f:\n",
    "            # summary sentences will first be written into the file in separate lines\n",
    "            with open('temporaryFile.txt', encoding = 'utf8') as t:\n",
    "                for line in t:\n",
    "                    line=line.lower()\n",
    "                    if line != \"\\n\" and line != \"\\t\" and line != \" \":\n",
    "                        f.write(b'@summary'+b'\\n')\n",
    "                        f.write(line.encode('utf-8'))\n",
    "                        f.write(b'\\n')\n",
    "                    \n",
    "            # finally the article is written to the file\n",
    "            f.write(b'@article' + b'\\n')    \n",
    "            f.write(article)\n",
    "\n",
    "title_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "favorite-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train Title/ Test Title/ Val Title\n",
    "url_train_title = 'https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_train.txt'\n",
    "train_title = pd.read_csv(url_train_title)\n",
    "\n",
    "url_test_title = 'https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_test.txt'\n",
    "test_title = pd.read_csv(url_test_title)\n",
    "\n",
    "url_val_title = 'https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_val.txt'\n",
    "val_title = pd.read_csv(url_val_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "variable-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length 168127\n",
      "test length 5999\n",
      "val length 5999\n"
     ]
    }
   ],
   "source": [
    "print('train length', len(train_title))\n",
    "print('test length', len(test_title))\n",
    "print('val length', len(val_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-montgomery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
